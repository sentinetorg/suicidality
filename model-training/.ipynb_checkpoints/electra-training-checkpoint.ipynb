{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328678be-9ee1-453e-9e63-fc89c5742e21",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7a3239a6-1b49-44b8-9afb-f59f7d243670",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SOURCE = \"datasets/extracted/dataset-cleaned.csv\"\n",
    "\n",
    "# TRAINING CONFIGS\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = 1e-5\n",
    "SEED = 7984\n",
    "\n",
    "MODEL_SAVE_PATH = \"models/sucidality\"\n",
    "MODEL_CHECKPOINT_PATH = \"models/sucidality-checkpoint\"\n",
    "MODEL_LOGGING_PATH = \"models/sucidality-checkpoint/logs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248b330-fb83-4290-b726-186b84c3bcf6",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6921014-9e6f-4fff-bc45-e347545c5426",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f33ac7-2790-46f9-86fd-cce261f6b8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/miniconda3/lib/python3.11/site-packages (4.31.0)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/miniconda3/lib/python3.11/site-packages (2.14.4)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/lib/python3.11/site-packages (1.25.2)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/1f/ad/9799aabeabcb9a293c87b6f96cc78655b8abc7d35560cd99007093b5d445/scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/b8/46/1d255bb55e63de02f7b2f3a2f71b59b840db21d61ff7cd41edbfc2da448a/scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from accelerate>=0.20.3->transformers) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from torch!=1.12.0,>=1.9->transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.9->transformers) (68.0.0)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.9->transformers) (0.41.1)\n",
      "Requirement already satisfied: cmake in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers) (3.27.2)\n",
      "Requirement already satisfied: lit in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/miniconda3/lib/python3.11/site-packages (from sympy->torch!=1.12.0,>=1.9->transformers) (1.3.0)\n",
      "Using cached scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.2 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.0 scipy-1.11.1 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers transformers[torch] datasets numpy pandas scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57728903-94d7-4f70-858e-6a28709dab22",
   "metadata": {},
   "source": [
    "## Import all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb84ef6-be41-4df0-9146-428c817a698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac53e2-6ba7-4c4e-b3a7-40afdc5d4886",
   "metadata": {},
   "source": [
    "## Load cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae935922-e66c-42cd-9810-0057544dbbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teenager nintendo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide default option abandonment issue issue...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>honestly not know anymore ina new school award...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want flip fucking chandler highway overpass fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lao life ruin figure loser deserve die no read...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262313</th>\n",
       "      <td>kind come party friend dog people no thank</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262314</th>\n",
       "      <td>see human film watch religiously help write es...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262315</th>\n",
       "      <td>coffee make sleepy suck</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262316</th>\n",
       "      <td>no pain pussy want die try kill not kill hang ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262317</th>\n",
       "      <td>mind previous post horny</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262017 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0                                       teenager nintendo      0\n",
       "1       suicide default option abandonment issue issue...      1\n",
       "2       honestly not know anymore ina new school award...      1\n",
       "3       want flip fucking chandler highway overpass fa...      0\n",
       "4       lao life ruin figure loser deserve die no read...      1\n",
       "...                                                   ...    ...\n",
       "262313         kind come party friend dog people no thank      0\n",
       "262314  see human film watch religiously help write es...      0\n",
       "262315                            coffee make sleepy suck      0\n",
       "262316  no pain pussy want die try kill not kill hang ...      1\n",
       "262317                           mind previous post horny      0\n",
       "\n",
       "[262017 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv(DATASET_SOURCE, lineterminator='\\n')\n",
    "ds = ds[['cleaned', 'class']]\n",
    "ds = ds.rename(columns={'cleaned': 'text', 'class': 'label'})\n",
    "ds = ds.dropna()\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f8454-e359-433a-a0ec-be6f55e10795",
   "metadata": {},
   "source": [
    "## Split dataset for training, evaluation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a61b1d6-9ff7-4bf5-86ec-746781b2d100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                     text  label\n",
       " 156594  consider buy gun want die live state get gun e...      1\n",
       " 116160  game thingie explain happen game lose billion ...      0\n",
       " 128769                                 feel sad feel well      0\n",
       " 1202    not understand college coach not run ball dump...      0\n",
       " 240691  want know bear pain want normal die tired way ...      1\n",
       " ...                                                   ...    ...\n",
       " 142070  jewish new year eve year tough wish good new y...      0\n",
       " 215373  vegan teacher got ban celebrate no soggy tit s...      0\n",
       " 85      wish relate read post help feel like write hop...      1\n",
       " 179933     get commitment issue try fix tell somebody get      0\n",
       " 153753             spineless moronic fucking matter tired      1\n",
       " \n",
       " [209613 rows x 2 columns],\n",
       "                                                      text  label\n",
       " 169025  fight boyfriend stay parent house not want sta...      1\n",
       " 138009  mon pass away past summer good friend move far...      1\n",
       " 246671            potato famine want mashed potato hungry      0\n",
       " 208491                                     have great day      0\n",
       " 103511  suddenly turn min craft skin screw llama hard ...      0\n",
       " ...                                                   ...    ...\n",
       " 207860                   want suck dick like no homo shit      0\n",
       " 194211  life bad not benefit rational euthanasia not m...      1\n",
       " 90073   sound sewer slide ill day tired exist tired ob...      1\n",
       " 74284   guy gal need help friend depressed want commit...      0\n",
       " 79876                          shit post writer block art      0\n",
       " \n",
       " [52404 rows x 2 columns],\n",
       "                                                      text  label\n",
       " 51481   get wisdom tooth today honestly want bad think...      0\n",
       " 228719  install shade rob lox play phantom force fukie...      0\n",
       " 239160  rude distant recently sorry nervous wreck like...      0\n",
       " 213179  high school graduation week not go college alc...      1\n",
       " 202638                                        go hang bye      1\n",
       " ...                                                   ...    ...\n",
       " 165620  texte crush good friend photo conversation old...      0\n",
       " 165483  week go gun kill doctor single work prison sex...      1\n",
       " 153026  real world man pretend not order win woman hea...      1\n",
       " 179367  go dinner abdul time meet heck nervous hope no...      0\n",
       " 6479    not enjoy anymore play video game watch show f...      1\n",
       " \n",
       " [26202 rows x 2 columns],\n",
       "                                                      text  label\n",
       " 8153    day planet earth love community decide tonight...      1\n",
       " 219656  girlfriend break text not text tell tell frien...      0\n",
       " 170883  lmaolmabsjdbbsjxbwnxhhdjsshdcmwbdjjf jrhdsndhd...      0\n",
       " 93249   hate revenue hate want die feel blood boiling ...      0\n",
       " 76659                             dog log dog log dog log      0\n",
       " ...                                                   ...    ...\n",
       " 163523  way able push suicide numb drug pill section t...      1\n",
       " 52006   bored fact d weird fact fillererereerususvsgwi...      0\n",
       " 219717  today dad die car accident family income drop ...      0\n",
       " 233539  no hope purpose bitter second feel purposeless...      1\n",
       " 50416   hate want die spend day hate hate feel like sh...      0\n",
       " \n",
       " [26202 rows x 2 columns])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, temp = train_test_split(ds, random_state=SEED, test_size=0.2, stratify=ds['label'])\n",
    "val, test = train_test_split(temp, random_state=SEED, test_size=0.5, stratify=temp['label'])\n",
    "\n",
    "train, temp, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28798e9e-15d4-4288-a4a7-d0afba3365c2",
   "metadata": {},
   "source": [
    "# Setup Model & tokenize the input texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a34647a-bea3-48d7-b688-f49d533679db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baeeeb6b-73a1-45d8-809b-9d8a93033d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gooohjy/suicidal-electra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a448be-de20-48b0-9231-c825884283de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 209613\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 26202\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 26202\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_conversion(train, test, val):\n",
    "  \"\"\"Converts pandas dataframe to Dataset.\"\"\"\n",
    "\n",
    "  train.reset_index(drop=True, inplace=True)\n",
    "  test.reset_index(drop=True, inplace=True)\n",
    "  val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  train_dataset = Dataset.from_pandas(train)\n",
    "  test_dataset = Dataset.from_pandas(test)\n",
    "  val_dataset = Dataset.from_pandas(val)\n",
    "\n",
    "  return DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"val\": val_dataset})\n",
    "\n",
    "raw_datasets = dataset_conversion(train, test, val)\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9f79d26-2325-41fd-af22-e9c8bdc11e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 209613/209613 [01:00<00:00, 3483.06 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26202/26202 [00:07<00:00, 3459.69 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26202/26202 [00:07<00:00, 3534.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 209613\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 26202\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 26202\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=False)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa384f-10c0-4683-8552-bb1982b090f0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f3d5f-a7eb-405d-a2ec-db0421d9c7fc",
   "metadata": {},
   "source": [
    "## Load the model & trainer methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e940d65d-6a1f-401e-811e-5b45257c0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"gooohjy/suicidal-electra\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c78f6757-ea6b-415a-9116-ce7fdee7dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom metrics for computation\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric_acc = load_metric(\"accuracy\")\n",
    "    metric_rec = load_metric(\"recall\")\n",
    "    metric_pre = load_metric(\"precision\")\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = metric_acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    recall = metric_rec.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    precision = metric_pre.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "\n",
    "    return {'accuracy': accuracy, 'recall': recall, 'precision': precision, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e16862a-abb0-4e01-834f-28db91d1b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and training parameters\n",
    "\n",
    "def get_trainer(datasets, epochs=EPOCHS):\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=MODEL_CHECKPOINT_PATH,\n",
    "        overwrite_output_dir = True,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        seed=SEED,\n",
    "        # evaluation_strategy=\"epoch\",\n",
    "        logging_dir=MODEL_LOGGING_PATH,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=1500\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets['train'],\n",
    "        eval_dataset=datasets['val'],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb30a1-70c9-40cb-9cef-b4c26b28e7c2",
   "metadata": {},
   "source": [
    "## Train a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "590ce91d-8984-4f2f-9873-507ab90fef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 20\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 20\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 20\n",
       " })}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 20\n",
    "\n",
    "sample = {\n",
    "    'train': tokenized_datasets['train'].shuffle(seed=SEED).select(range(SAMPLE_SIZE)),\n",
    "    'test': tokenized_datasets['test'].shuffle(seed=SEED).select(range(SAMPLE_SIZE)),\n",
    "    'val': tokenized_datasets['val'].shuffle(seed=SEED).select(range(SAMPLE_SIZE))\n",
    "}\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c64cd83-0ac7-4f0f-9616-d95d76c8b842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=1.2119610346417175e-07, metrics={'train_runtime': 3.1428, 'train_samples_per_second': 31.819, 'train_steps_per_second': 6.364, 'total_flos': 26311105536000.0, 'train_loss': 1.2119610346417175e-07, 'epoch': 5.0})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = get_trainer(sample)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5dfaf187-e65a-44e8-8f4e-3941604416e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE TEST RESULTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.842932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_recall</th>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_precision</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_f1</th>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>0.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>41.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>8.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         SAMPLE TEST RESULTS\n",
       "eval_loss                           0.842932\n",
       "eval_accuracy                       0.950000\n",
       "eval_recall                         0.909091\n",
       "eval_precision                      1.000000\n",
       "eval_f1                             0.952381\n",
       "eval_runtime                        0.478900\n",
       "eval_samples_per_second            41.760000\n",
       "eval_steps_per_second               8.352000\n",
       "epoch                               5.000000"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = trainer.evaluate()\n",
    "\n",
    "pd.DataFrame(data=result, index=['SAMPLE TEST RESULTS']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba69f12-8419-4e36-a68e-085c60a0c9c1",
   "metadata": {},
   "source": [
    "## Train model with the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122fb3f-8913-4d04-b43d-a7b9b588d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2261' max='174680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2261/174680 06:57 < 8:50:29, 5.42 it/s, Epoch 0.06/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.342100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = get_trainer(tokenized_datasets)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66f8f3-52ac-4ecb-a455-621b5c36c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.evaluate()\n",
    "pd.DataFrame(data=result, index=['SAMPLE TEST RESULTS']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036b67e-e903-46fa-bd47-8b5fac201343",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ed21f-c392-4d80-8c04-8030a75da593",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
